{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing for Machine learning Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of dropping na and columns\n",
    "\n",
    "# Drop the Latitude and Longitude columns from volunteer\n",
    "volunteer_cols = volunteer.drop(['Latitude' , 'Longitude'] , axis = 1)\n",
    "\n",
    "# Drop rows with missing category_desc values from volunteer_cols\n",
    "volunteer_subset = volunteer_cols.dropna(subset = ['category_desc'])\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of converting data types\n",
    "\n",
    "# Print the head of the hits column\n",
    "print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer[\"hits\"].astype('int')\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "print(volunteer.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split in a dataset with a class imbalance (stratified- y)\n",
    "\n",
    "# Create a DataFrame with all columns except category_desc\n",
    "X = volunteer.drop('category_desc', axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "y = volunteer[['category_desc']]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, random_state=42)\n",
    "\n",
    "# Print the category_desc counts from y_train\n",
    "print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardization is a preprocessing method to transform numerical continuous daata to make it look normally distributed\n",
    "\n",
    "log normalization and scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the model operates at linear space. eg k-Nearest Kneighbors , Linear reg , K-Means clustering. or has if data has high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of modelling a high cariance column without standardization in a dataset\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the knn model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test , y_test))\n",
    "\n",
    "# outputs 0.6888"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log normalization can be used for the dataset above with high varience\n",
    "\n",
    "Applies logarithmic transformation - captures relative changes, magnitude of change - keeps everything positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of transformation of a 'wine ' dataset\n",
    "# \n",
    "# # Print out the variance of the Proline column\n",
    "print(wine[\"Proline\"].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine[\"Proline_log\"].var())\n",
    "\n",
    "# var output of proline is 99166 and drops to 0.17 upon standardization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaling for features on different scales if model with linear characteristics is used. Centres features around 0 and transform to variance one (approx normal distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### example of standardscaler \n",
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Subset the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash' , 'Alcalinity of ash' , 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to wine_subset\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking the difference - with and without standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WITHOUT STANDARDIZATION\n",
    "\n",
    "# Split the dataset and labels into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "\n",
    "# OUTPUTS 0.777\n",
    "\n",
    "\n",
    "#### WITH STANDARDIZATION\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Instantiate a StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training and test features\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Fit the k-nearest neighbors model to the training data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test_scaled, y_test))\n",
    "\n",
    "# OUTPUTS 0.933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering. Creation of new features from existing ones to improve performance, insight for relationships between features.\n",
    "\n",
    "encoding categorical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF LABEL ENCODING \n",
    "# Set up the LabelEncoder object\n",
    "enc = LabelEncoder()\n",
    "\n",
    "# Apply the encoding to the \"Accessible\" column\n",
    "hiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n",
    "\n",
    "# Compare the two columns\n",
    "print(hiking[[\"Accessible_enc\", \"Accessible\"]].head())\n",
    "\n",
    "\n",
    "# <script.py> output:\n",
    "#        Accessible_enc Accessible\n",
    "#     0               1          Y\n",
    "#     1               0          N\n",
    "#     2               0          N\n",
    "#     3               0          N\n",
    "#     4               0          N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF ONE HOT ENCODING\n",
    "\n",
    "# Transform the category_desc column\n",
    "category_enc = pd.get_dummies(volunteer['category_desc'])\n",
    "\n",
    "# Take a look at the encoded columns\n",
    "print(category_enc.head())\n",
    "\n",
    "    #    Education  Emergency Preparedness  Environment  Health  Helping Neighbors in Need  Strengthening Communities\n",
    "    # 0          0                       0            0       0                          0                          0\n",
    "    # 1          0                       0            0       0                          0                          1\n",
    "    # 2          0                       0            0       0                          0                          1\n",
    "    # 3          0                       0            0       0                          0                          1\n",
    "    # 4          0                       0            1       0                          0                          0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating numerical features. - take descriptive stats as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .loc to create a mean column\n",
    "running_times_5k[\"mean\"] = running_times_5k.loc[:, :].mean(axis=1)\n",
    "\n",
    "# Take a look at the results\n",
    "print(running_times_5k.head())\n",
    "# <script.py> output:\n",
    "#         name  run1  run2  run3  run4  run5   mean\n",
    "#     0    Sue  20.1  18.5  19.6  20.3  18.3  19.36\n",
    "#     1   Mark  16.5  17.1  16.9  17.6  17.3  17.08\n",
    "#     2   Sean  23.5  25.1  25.2  24.6  23.9  24.46\n",
    "#     3   Erin  21.7  21.1  20.9  22.1  22.2  21.60\n",
    "#     4  Jenny  25.8  27.1  26.1  26.7  26.9  26.52\n",
    "\n",
    "\n",
    "\n",
    "# Extracting a month from a date using datetime\n",
    "\n",
    "# First, convert string column to date column\n",
    "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer['start_date_date'])\n",
    "\n",
    "# Extract just the month from the converted column\n",
    "volunteer[\"start_date_month\"] = volunteer['start_date_converted'].dt.month\n",
    "\n",
    "# Take a look at the converted and new month columns\n",
    "print(volunteer[['start_date_converted', 'start_date_month']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting string patterns\n",
    "\n",
    "The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern to extract numbers and decimals\n",
    "def return_mileage(length):\n",
    "    \n",
    "    # Search the text for matches\n",
    "    mile = re.search(\"\\d+\\.\\d+\", length)\n",
    "    \n",
    "    # If a value is returned, use group(0) to return the found value\n",
    "    if mile is not None:\n",
    "        return float(mile.group(0))\n",
    "        \n",
    "# Apply the function to the Length column and take a look at both columns\n",
    "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
    "print(hiking[[\"Length\", \"Length_num\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature selection\n",
    "\n",
    "removing features that are noisy, correlated, or duplicates.\n",
    "\n",
    "correlated features may induce bias in linear models. Use Pearson's correlation to check a feature set for correlation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of redundant column names to drop\n",
    "to_drop = [\"category_desc\", \"created_date\", \"locality\", \"region\", \"vol_requests\"]\n",
    "\n",
    "# Drop those columns from the dataset\n",
    "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
    "\n",
    "# Print out the head of volunteer_subset\n",
    "print(volunteer_subset.head())\n",
    "\n",
    "# dropping columns with corr higher than 0.75 \n",
    "# Print out the column correlations of the wine dataset\n",
    "print(wine.corr())\n",
    "\n",
    "# Drop that column from the DataFrame\n",
    "wine = wine.drop(\"Flavanoids\", axis=1)\n",
    "\n",
    "print(wine.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selecting features using text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of the arguments\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n",
    "\n",
    "\n",
    "\n",
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and dimensionality reduction\n",
    "\n",
    "feature extraction method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a PCA object\n",
    "pca = PCA()\n",
    "\n",
    "# Define the features and labels from the wine dataset\n",
    "X = wine.drop('Type', axis =1)\n",
    "y = wine[\"Type\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Apply PCA to the wine dataset X vector\n",
    "pca_X_train = pca.fit_transform(X_train)\n",
    "pca_X_test = pca.transform(X_test)\n",
    "\n",
    "# Look at the percentage of variance explained by the different components\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_minutes(time_string):\n",
    "    \n",
    "    # Search for numbers in time_string\n",
    "    num = re.search(\"\\d+\", time_string)\n",
    "    if num is not None:\n",
    "        return int(num.group(0))\n",
    "        \n",
    "# Apply the extraction to the length_of_time column\n",
    "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
    "\n",
    "# Take a look at the head of both of the columns\n",
    "print(ufo[[\"length_of_time\", \"minutes\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling missing data part II\n",
    "\n",
    "Imputation\n",
    "can reduce the probability of introducing bias and most ML algos require complete data. Effects depend on missing values, original variance, outliers and size and direction of skew.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with zero\n",
    "loan_data_filled = loan_data.fillna(0)\n",
    "\n",
    "# Examine 'Credit Score' before\n",
    "print(loan_data['Credit Score'].describe())\n",
    "\n",
    "# Examine 'Credit Score' after\n",
    "print(loan_data_filled['Credit Score'].describe())\n",
    "\n",
    "# running the above code reduces the dataset way too much\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE SIMPLE IMPUTER\n",
    "# Import imputer module\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Subset numeric features: numeric_cols\n",
    "numeric_cols = loan_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Impute with mean\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "loans_imp_mean = imp_mean.fit_transform(numeric_cols)\n",
    "\n",
    "# Convert returned array to DataFrame\n",
    "loans_imp_meanDF = pd.DataFrame(loans_imp_mean, columns=numeric_cols.columns)\n",
    "\n",
    "# Check the DataFrame's info\n",
    "print(loans_imp_meanDF.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly require this experimental feature\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "# Now you can import normally from sklearn.impute\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Subset numeric features: numeric_cols\n",
    "numeric_cols = loan_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# Iteratively impute\n",
    "imp_iter = IterativeImputer(max_iter=3, random_state=123)\n",
    "loans_imp_iter = imp_iter.fit_transform(numeric_cols)\n",
    "\n",
    "# Convert returned array to DataFrame\n",
    "loans_imp_iterDF = pd.DataFrame(loans_imp_iter, columns=numeric_cols.columns)\n",
    "\n",
    "# Check the DataFrame's info\n",
    "print(loans_imp_iterDF.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA DISTRIBUTIONS AND TRANSFORMATIONS\n",
    "if the training and test sets have different distributions // diff mean and vars this will lead to poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSPECTING the training test sets\n",
    "\n",
    "# Create `loan_data` subset: loan_data_subset\n",
    "loan_data_subset = loan_data[['Credit Score','Annual Income','Loan Status']]\n",
    "\n",
    "# Create train and test sets\n",
    "trainingSet, testSet = train_test_split(loan_data, test_size=0.2, random_state=123)\n",
    "\n",
    "# Examine pairplots\n",
    "plt.figure()\n",
    "sns.pairplot(trainingSet, hue='Loan Status', palette='RdBu')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "sns.pairplot(testSet, hue='Loan Status', palette='RdBu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above shows that the test and training data show different distributions which may violate normality assumptions in several models.\n",
    "\n",
    "Log and power transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset loan_data\n",
    "cr_yrs = loan_data['Years of Credit History']\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "plt.figure()\n",
    "sns.distplot(cr_yrs)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Subset loan_data\n",
    "cr_yrs = loan_data['Years of Credit History']\n",
    "\n",
    "# Square root transform\n",
    "cr_yrs_sqrt = boxcox(cr_yrs, lmbda=0.5)\n",
    "\n",
    "# Histogram and kernel density estimate\n",
    "plt.figure()\n",
    "sns.distplot(cr_yrs_sqrt)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data outliers and scaling\n",
    " less than Q1 - 1.5*IQR and higher than Q3 +1/5 *IQR\n",
    "\n",
    " visualizing can be good in detecting outliers\n",
    "boxplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Univariate and multivariate boxplots\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(y=loan_data['Annual Income'], ax=ax[0])\n",
    "sns.boxplot(x='Loan Status', y='Annual Income', data=loan_data, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "# Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Multivariate boxplot\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(y=loan_data['Monthly Debt'], ax=ax[0])\n",
    "sns.boxplot(x='Loan Status', y='Monthly Debt', data=loan_data, ax=ax[1])\n",
    "plt.show()\n",
    "\n",
    "# Import modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Univariate and multivariate boxplots\n",
    "fig, ax =plt.subplots(1,2)\n",
    "sns.boxplot(x=loan_data['Years of Credit History'], ax=ax[0])\n",
    "sns.boxplot(x='Loan Status', y='Years of Credit History', data=loan_data, ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all of the above contain outliers that need to be dealt with\n",
    "one way is to calculate the z-score which gives a threshold for outliers +/- 3 sigmas away from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stats.zscore() and mstats.winsorize to replace outliers\n",
    "# Print: before dropping\n",
    "print(numeric_cols.mean())\n",
    "print(numeric_cols.median())\n",
    "print(numeric_cols.max())\n",
    "\n",
    "# Create index of rows to keep\n",
    "# creates an array of true false if all values in the row have a z-score less than 3\n",
    "idx = (np.abs(stats.zscore(numeric_cols)) < 3).all(axis=1)\n",
    "\n",
    "# Concatenate numeric and categoric subsets\n",
    "ld_out_drop = pd.concat([numeric_cols.loc[idx], categoric_cols.loc[idx]], axis=1)\n",
    "\n",
    "# Print: after dropping\n",
    "print(ld_out_drop.mean())\n",
    "print(ld_out_drop.median())\n",
    "print(ld_out_drop.max())\n",
    "\n",
    "\n",
    "## WINSORIZE\n",
    "# Print: before winsorize\n",
    "print((loan_data['Monthly Debt']).mean())\n",
    "print((loan_data['Monthly Debt']).median())\n",
    "print((loan_data['Monthly Debt']).max())\n",
    "\n",
    "# Winsorize numeric columns\n",
    "debt_win = mstats.winsorize(loan_data['Monthly Debt'], limits=[0.05, 0.05])\n",
    "\n",
    "# Convert to DataFrame, reassign column name\n",
    "debt_out = pd.DataFrame(debt_win, columns=['Monthly Debt'])\n",
    "\n",
    "# Print: after winsorize\n",
    "print(debt_out.mean())\n",
    "print(debt_out.median())\n",
    "print(debt_out.max())\n",
    "\n",
    "\n",
    "\n",
    "# Print: before replace with median\n",
    "print((loan_data['Monthly Debt']).mean())\n",
    "print((loan_data['Monthly Debt']).median())\n",
    "print((loan_data['Monthly Debt']).max())\n",
    "\n",
    "# Find median\n",
    "median = loan_data.loc[loan_data['Monthly Debt'] < 2120, 'Monthly Debt'].median()\n",
    "loan_data['Monthly Debt'] = np.where(loan_data['Monthly Debt'] > 2120, median, loan_data['Monthly Debt'])\n",
    "\n",
    "# Print: after replace with median\n",
    "print((loan_data['Monthly Debt']).mean())\n",
    "print((loan_data['Monthly Debt']).median())\n",
    "print((loan_data['Monthly Debt']).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z-score standardization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting numeric and categorical variables\n",
    "# then standard scaling the numeric and making the final dataframe\n",
    "\n",
    "# Subset features\n",
    "numeric_cols = loan_data.select_dtypes(include=[np.number])\n",
    "categoric_cols = loan_data.select_dtypes(include=[object])\n",
    "\n",
    "# Subset features\n",
    "numeric_cols = loan_data.select_dtypes(include=[np.number])\n",
    "categoric_cols = loan_data.select_dtypes(include=[object])\n",
    "\n",
    "# Instantiate\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform, convert to DF\n",
    "numeric_cols_scaled = scaler.fit_transform(numeric_cols)\n",
    "numeric_cols_scaledDF = pd.DataFrame(numeric_cols_scaled, columns=numeric_cols.columns)\n",
    "\n",
    "# Concatenate categoric columns to scaled numeric columns\n",
    "final_DF = pd.concat([numeric_cols_scaledDF, categoric_cols], axis = 1)\n",
    "print(final_DF.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to best select features for a linear regresison model. reduces overfitting by contributing noise but not info. misleading data is removed. less complex - more interpretable. less time to train\n",
    "\n",
    "$ main types\n",
    "filter: rank features based on statistical performance\n",
    "Wrapper: use an ml method to evaluate performance\n",
    "embedded: iterative model traing to extract features.\n",
    "importance: tree-based ML models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter type:\n",
    "continuous - continuous : pearsons correlation\n",
    "continuous- categorical : ANOVA ,LDA\n",
    "categorical- categorical : chi squ\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter method practice\n",
    "\n",
    "# Create correlation matrix and print it\n",
    "cor = diabetes.corr()\n",
    "print(cor)\n",
    "\n",
    "# Correlation matrix heatmap\n",
    "plt.figure()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
    "plt.show()\n",
    "\n",
    "# Correlation with output variable\n",
    "cor_target = abs(cor[\"progression\"])\n",
    "\n",
    "# Selecting highly correlated features\n",
    "best_features = cor_target[cor_target > 0.5]\n",
    "print(best_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM and RFECV fitting\n",
    "# wrapper method\n",
    "\n",
    "# Import modules\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Instantiate estimator and feature selector\n",
    "svr_mod = SVR(kernel=\"linear\")\n",
    "feat_selector =RFECV(svr_mod, cv=5)\n",
    "\n",
    "# Fit\n",
    "feat_selector = feat_selector.fit(X, y)\n",
    "\n",
    "# Print support and ranking\n",
    "print(feat_selector.support_)\n",
    "print(feat_selector.ranking_)\n",
    "print(X.columns)\n",
    "\n",
    "# Import modules\n",
    "from sklearn.linear_model import LarsCV\n",
    "\n",
    "# Drop feature suggested not important in step 2\n",
    "X = X.drop('sex', axis=1)\n",
    "\n",
    "# Instantiate\n",
    "lars_mod = LarsCV(cv=5, normalize=False)\n",
    "\n",
    "# Fit\n",
    "feat_selector = lars_mod.fit(X, y)\n",
    "\n",
    "# Print r-squared score and estimated alpha\n",
    "print(lars_mod.score(X, y))\n",
    "print(lars_mod.alpha_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree- based methods can be even better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatiating a random forest regressor\n",
    "# Import\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate\n",
    "rf_mod = RandomForestRegressor(max_depth=2, random_state=123, \n",
    "              n_estimators=100, oob_score=True)\n",
    "\n",
    "# Fit\n",
    "rf_mod.fit(X, y)\n",
    "\n",
    "# Print\n",
    "print(diabetes.columns)\n",
    "print(rf_mod.feature_importances_)\n",
    "# extra trees regressor\n",
    "\n",
    "# Import\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "# Instantiate\n",
    "xt_mod = ExtraTreesRegressor()\n",
    "\n",
    "# Fit\n",
    "xt_mod.fit(X, y)\n",
    "\n",
    "# Print\n",
    "print(diabetes.columns)\n",
    "print(xt_mod.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods: ridge regression lasso regression and elastic net. Regularization: reduce model complexity and prevent overfitting. they do this by adding a penalty term to the OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso using lassoCV to get alpha\n",
    "# Import modules\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "# Instantiate cross-validated lasso, fit\n",
    "lasso_cv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate lasso, fit, predict and print MSE\n",
    "lasso = Lasso(alpha = lasso_cv.alpha_)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(mean_squared_error(y_true=y_test, y_pred=lasso.predict(X_test)))\n",
    "\n",
    "\n",
    "# Import modules\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "# Instantiate cross-validated ridge, fit\n",
    "ridge_cv = RidgeCV(alphas=np.logspace(-6, 6, 13))\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate ridge, fit, predict and print MSE\n",
    "ridge = Ridge(alpha = ridge_cv.alpha_)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(mean_squared_error(y_true=y_test, y_pred=ridge.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification- feature engineering \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and predict a Logistic Regression on loan_data with the target variable Loan Status as y and evaluate the trained model's accuracy score.\n",
    "\n",
    "# Create X matrix and y array\n",
    "X = loan_data.drop(\"Loan Status\", axis=1)\n",
    "y = loan_data[\"Loan Status\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Instantiate\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "# Fit\n",
    "logistic.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "print(accuracy_score(y_true=y_test, y_pred=logistic.fit(X_test)))\n",
    "\n",
    "\n",
    "#Convert Annual Income to monthly, and derive the ratio of Monthly Debt to monthly_income and store it in dti_ratio\n",
    "# Convert income: monthly_income\n",
    "monthly_income = loan_data[\"Annual Income\"]/12\n",
    "\n",
    "\n",
    "#Convert the target variable to numerical values and replace categorical features with dummy values.\n",
    "# Create dti_ratio variable\n",
    "monthly_income = loan_data[\"Annual Income\"]/12\n",
    "loan_data[\"dti_ratio\"] = loan_data[\"Monthly Debt\"]/monthly_income * 100\n",
    "loan_data = loan_data.drop([\"Monthly Debt\",\"Annual Income\"], axis=1)\n",
    "\n",
    "# Replace target variable levels\n",
    "loan_data[\"Loan Status\"] = loan_data[\"Loan Status\"].replace({'Fully Paid': 0, \n",
    "                                                             'Charged Off': 1})\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "loan_data = pd.get_dummies(data=loan_data)\n",
    "\n",
    "# Print\n",
    "print(loan_data.head())\n",
    "\n",
    "# Fit and predict a Logistic Regression on loans_dti and evaluate the trained model's accuracy score.\n",
    "\n",
    "# Create X matrix and y array\n",
    "X = loans_dti.drop(\"Loan Status\", axis=1)\n",
    "y = loans_dti[\"Loan Status\"]\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Instantiate\n",
    "logistic_dti = LogisticRegression()\n",
    "\n",
    "# Fit\n",
    "logistic_dti.fit(X_train, y_train)\n",
    "\n",
    "# Predict and print accuracy\n",
    "print(accuracy_score(y_true=y_test, y_pred=logistic_dti.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensemble methods\n",
    "\n",
    "Bootsrap aggregation, boosting, model stacking\n",
    "\n",
    "BAGGING - subset selected with replacement . model built for each sample - average the output. reduces variance improves models.\n",
    "\n",
    "Boosting - multiple models built sequencially. incorrect predictions are weighted - reduces bias\n",
    "\n",
    "model stacking - model 1 ,2 -N  trained individually and then stacked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate bootstrap aggregation model\n",
    "bagged_model = BaggingClassifier(n_estimators=50, random_state=123)\n",
    "\n",
    "# Fit\n",
    "bagged_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "bagged_pred = bagged_model.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "print(accuracy_score(y_test, bagged_pred))\n",
    "\n",
    "#-- Adaboost example\n",
    "\n",
    "# Boosting model\n",
    "boosted_model = AdaBoostClassifier(n_estimators=50, random_state=123)\n",
    "\n",
    "# Fit\n",
    "boosted_model_fit = boosted_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "boosted_pred = boosted_model_fit.predict(X_test)\n",
    "\n",
    "# Print model accuracy\n",
    "print(accuracy_score(y_test, boosted_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost - The argument learning_rate=0.1 specifies the size of the step to take in each iteration while searching for the global minimum and max_depth controls the size (depth) of the decision trees, here 3.\n",
    "# Instantiate\n",
    "xgb = XGBClassifier(random_state=123, learning_rate=0.1, n_estimators=10, max_depth=3)\n",
    "\n",
    "# Fit\n",
    "xgb = xgb.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "xgb_pred = xgb.predict(X_test)\n",
    "\n",
    "# Print accuracy score\n",
    "print('Final prediction score: [%.8f]' % accuracy_score(y_test, xgb_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA and SVD in unsupervised learning. model performance decreases as the number of features increases - curse of dimensionality. perform dimensionality reduction to prevent overfitting.\n",
    "\n",
    "PCA - calculated by finding principle axes. translates rotates or scales the data. second pc is a direction perpendicular of the first. lower dimensional projection of the data containing the maximal amount of variance.\n",
    "\n",
    "SVD- decomposes data matrix into three matrices. singular values. \n",
    "the sum of the squares f singular values should approximate the total variance of the original matrix.\n",
    "\n",
    "speed ml training , improve accuracy visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Feature matrix and target array\n",
    "X = diabetes.drop('progression', axis=1)\n",
    "y = diabetes['progression']\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "# Fit and transform\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "# Print ratio of variance explained\n",
    "print(pca.explained_variance_ratio_)\n",
    "\n",
    "# Import module\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Feature matrix and target array\n",
    "X = diabetes.drop('progression', axis=1)\n",
    "y = diabetes['progression']\n",
    "\n",
    "# SVD\n",
    "svd = TruncatedSVD(n_components=3)\n",
    "\n",
    "# Fit and transform\n",
    "principalComponents = svd.fit_transform(X)\n",
    "\n",
    "# Print ratio of variance explained\n",
    "print(svd.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-stochastical neighbor embedding t-sne\n",
    "takes pairs of data points , computes the probability they are related and chooses a low dimensional embedding to produce a similar distribution. this can be plot. a lot of computing power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [0, 1]\n",
    "colors = ['r', 'b']\n",
    "\n",
    "# For loop to create plot\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = loan_data_PCA['Loan Status'] == target\n",
    "    ax.scatter(loan_data_PCA.loc[indicesToKeep, 'principal component 1']\n",
    "               , loan_data_PCA.loc[indicesToKeep, 'principal component 2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "\n",
    "# Legend    \n",
    "ax.legend(targets)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Remove target variable\n",
    "X = loan_data.drop('Loan Status', axis=1)\n",
    "\n",
    "# Instantiate\n",
    "pca = PCA(n_components=10)\n",
    "\n",
    "# Fit and transform\n",
    "principalComponents = pca.fit_transform(X)\n",
    "\n",
    "# List principal components names\n",
    "principal_components = ['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10']\n",
    "\n",
    "# Create a DataFrame\n",
    "pca_df = pd.DataFrame({'Variance Explained': pca.explained_variance_ratio_,\n",
    "             'PC':principal_components})\n",
    "\n",
    "# Plot DataFrame\n",
    "sns.barplot(x='PC',y='Variance Explained', \n",
    "           data=pca_df, color=\"c\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Instantiate, fit and transform\n",
    "pca2 = PCA()\n",
    "principalComponents2 = pca2.fit_transform(X)\n",
    "\n",
    "# Assign variance explained\n",
    "var = pca2.explained_variance_ratio_\n",
    "\n",
    "# Plot cumulative variance\n",
    "cumulative_var = np.cumsum(var)*100\n",
    "plt.plot(cumulative_var,'k-o',markerfacecolor='None',markeredgecolor='k')\n",
    "plt.title('Principal Component Analysis',fontsize=12)\n",
    "plt.xlabel(\"Principal Component\",fontsize=12)\n",
    "plt.ylabel(\"Cumulative Proportion of Variance Explained\",fontsize=12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clustering analysis. rely on distance calculations. \n",
    "\n",
    "k means- choosing the initial centroids. each observation is assigned to its nearest centroid followed by taking the mean of all for a given centroid to create new centroids. iterated until the centroids do not move.\n",
    "\n",
    "hierarchical agglomerative clustering.successively merging or splitting observations. the hierarchy represented as a tree known as a dendrogram. Bottom-up approach. each observation starts in its own cluster, becoming merged into groups of clusters based on merging criteria.depends on linkage criteria and distance threshold\n",
    "\n",
    "ward linkage is like eucledian. maximum/ complete linkage minimizes the average of the distances between obs in pairs of clusters.averages and single\n",
    "\n",
    "cluster stability assessment -\n",
    "intracluster distane - mean of the distances between the points of a cluster and its centroid\n",
    "\n",
    "intercluster distance - mean of the distances between cluster's centroids\n",
    "\n",
    "intra should be less than the inter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create feature matrix\n",
    "X = diabetes.drop(\"progression\", axis=1)\n",
    "\n",
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=2, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 2 clusters is\", kmeans.inertia_)\n",
    "\n",
    "\n",
    "# Import module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create feature matrix\n",
    "X = diabetes.drop(\"progression\", axis=1)\n",
    "\n",
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=5, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 5 clusters is\", kmeans.inertia_)\n",
    "\n",
    "\n",
    "# Import module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create feature matrix\n",
    "X = diabetes.drop(\"progression\", axis=1)\n",
    "\n",
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 10 clusters is\", kmeans.inertia_)\n",
    "\n",
    "\n",
    "# Import module\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create feature matrix\n",
    "X = diabetes.drop(\"progression\", axis=1)\n",
    "\n",
    "# Instantiate\n",
    "kmeans = KMeans(n_clusters=10, random_state=123)\n",
    "\n",
    "# Fit\n",
    "fit = kmeans.fit(X)\n",
    "\n",
    "# Print inertia\n",
    "print(\"Sum of squared distances for 10 clusters is\", kmeans.inertia_)\n",
    "\n",
    "#20 returns even lower inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hierarchical clustering libraries\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Create dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Import hierarchical clustering libraries\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Create dendrogram\n",
    "dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n",
    "plt.show()\n",
    "\n",
    "# Create clusters and fit\n",
    "hc = AgglomerativeClustering(affinity = 'euclidean', linkage = 'ward')\n",
    "hc.fit(X)\n",
    "# Print number of clusters\n",
    "print(hc.n_clusters_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choosing the optimal number of clusters.\n",
    "\n",
    "Silhouette method \n",
    "elbow method\n",
    "\n",
    "silhouette coeff is composed of two scores. mean distance between each obs and all others.\n",
    "This is for the SAME and NEAREST clusters. the value ranges between -1 and 1 with (1) indicating that the obs is very near others in the cluster and far from obs in other clusters. 0 denotes overlapping clusters. and -1 shows that the obs is not near to others in the same cluster but close to other obs in other clusters.\n",
    "\n",
    "\n",
    "The elbow method is simply a visualization technique that if the plot of the total sum of square distance against number of clusters looks like an arm, then elbow is optimal k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Feature matrix\n",
    "X = diabetes.drop(\"progression\", axis=1)\n",
    "\n",
    "# For loop\n",
    "for n_clusters in range(2,9):\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    # Fit and predict your k-Means object\n",
    "    preds = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, preds, metric='euclidean')\n",
    "    print (\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))\n",
    "\n",
    "\n",
    "\n",
    "# Create empty list\n",
    "sum_of_squared_distances = []\n",
    "\n",
    "# Create for loop\n",
    "for k in range(1,15):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans = kmeans.fit(X)\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(1,15), sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bootstrapping is for method generalization.    \n",
    "eg. take a decision tree. it is easy to understand and visualize but can be easily overfit and can be greedy. aslo biased in cases of class imbalance\n",
    "\n",
    "a random forrest is a bootstrapped version of many decision trees. averages the output predictions to reduce variance giving a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k fold cross-val is another tool that can help generilize models by preventing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)\n",
    "\n",
    "# Instantiate, Fit, Predict\n",
    "loans_clf = DecisionTreeClassifier() \n",
    "loans_clf.fit(X_train , y_train)\n",
    "y_pred = loans_clf.predict(X_test)\n",
    "\n",
    "# Evaluation metric\n",
    "print(\"Decision Tree Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "# Import modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_grid = {\"criterion\": [\"gini\"], \"min_samples_split\": [2, 10, 20], \n",
    "              \"max_depth\": [None, 2, 5, 10]}\n",
    "\n",
    "# Instantiate classifier and GridSearchCV, fit\n",
    "loans_clf = DecisionTreeClassifier()\n",
    "dtree_cv = GridSearchCV(loans_clf, param_grid = param_grid, cv=5)\n",
    "fit = dtree_cv.fit(X_train,y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Decision Tree Parameter: {}\".format(dtree_cv.best_params_))\n",
    "print(\"Tuned Decision Tree Accuracy: {}\".format(dtree_cv.best_score_))\n",
    "\n",
    "# Tuned Decision Tree Parameter: {'criterion': 'gini', 'max_depth': 2, 'min_samples_split': 2}\n",
    "#     Tuned Decision Tree Accuracy: 0.7014285714285714\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=123)\n",
    "\n",
    "# Instantiate, Fit, Predict\n",
    "loans_rf = RandomForestClassifier() \n",
    "loans_rf.fit(X_train,y_train)\n",
    "y_pred = loans_rf.predict(X_test)\n",
    "\n",
    "# Evaluation metric\n",
    "print(\"Random Forest Accuracy: {}\".format(accuracy_score(y_test,y_pred)))\n",
    "\n",
    "# Import modules\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create the hyperparameter grid\n",
    "param_grid = {\"criterion\": [\"gini\"], \"min_samples_split\": [2, 10, 20], \n",
    "              \"max_depth\": [None, 2, 5, 10],\"max_features\": [10, 20, 30]}\n",
    "\n",
    "# Instantiate classifier and GridSearchCV, fit\n",
    "loans_rf = RandomForestClassifier(n_estimators=10)\n",
    "rf_cv = GridSearchCV(loans_rf, param_grid, cv=3)\n",
    "fit = rf_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal parameters and best score\n",
    "print(\"Tuned Random Forest Parameter: {}\".format(rf_cv.best_params_))\n",
    "print(\"Tuned Random Forest Accuracy: {}\".format(rf_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class imbalance - most algos are designed to min error but maximise acc. can cause misleading results. confusion matrix.\n",
    "\n",
    "-accuracy (TN +TP)/(TN+TP+FN+FP)\n",
    "-precision (TP) / (FP +TP)\n",
    "measures how often model is correct for positive class\n",
    "\n",
    "-sensitivity TP / (TP+ FN)\n",
    "measure of how often a positive is predicted when truly positive\n",
    "\n",
    "-specificity TN /(TN+FP)\n",
    "-f1 = 2* (precision * sensitivity)/ (precision +sensitivity)\n",
    "f1 is the weighted average of the precision and recall\n",
    "\n",
    "when evaluating a dataset with imbalanced classes, accuracy should not be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resampling to solve imbalance. oversample minority class or undersample majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "lr = LogisticRegression(solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, y_pred)))\n",
    "\n",
    "    #  [[  367  3868]\n",
    "    #  [  318 10447]]\n",
    "    # Accuracy: 0.7209333333333333\n",
    "    # Precision: 0.7297939224589591\n",
    "    # Recall: 0.9704598235020901\n",
    "    # F1: 0.8330940988835726\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling \n",
    "# Upsample minority and combine with majority\n",
    "loans_upsampled = resample(deny, replace=True, n_samples=len(approve), random_state=123)\n",
    "upsampled = pd.concat([approve, loans_upsampled])\n",
    "\n",
    "# Downsample majority and combine with minority\n",
    "loans_downsampled = resample(approve, replace = False,  n_samples = len(deny), random_state = 123)\n",
    "downsampled = pd.concat([loans_downsampled, deny])\n",
    "\n",
    "\n",
    "# Upsampled feature matrix and target array\n",
    "X_train_up = upsampled.drop('Loan Status', axis=1)\n",
    "y_train_up = upsampled['Loan Status']\n",
    "\n",
    "# Instantiate logistic regression, fit, predict\n",
    "loan_lr_up = LogisticRegression(solver='liblinear')\n",
    "loan_lr_up.fit(X_train_up, y_train_up)\n",
    "upsampled_y_pred = loan_lr_up.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, upsampled_y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, upsampled_y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, upsampled_y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, upsampled_y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, upsampled_y_pred)))\n",
    "\n",
    "# Downsampled feature matrix and target array\n",
    "X_train_down = downsampled.drop('Loan Status', axis=1)\n",
    "y_train_down = downsampled['Loan Status']\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "loan_lr_down = LogisticRegression(solver='liblinear')\n",
    "loan_lr_down.fit(X_train_down, y_train_down)\n",
    "downsampled_y_pred = loan_lr_down.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, downsampled_y_pred)))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_test, downsampled_y_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, downsampled_y_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, downsampled_y_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, downsampled_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multicollinearity : high correlation between independent variable. estimated regression coefficients (change in dependent var explained by the independent var while holding others constant).\n",
    "when independent vars are correlated, interpreting the amount of explained var is less clear.\n",
    "\n",
    "multicollinearity results in reducing coefficients , reducing p-values and unstable variance, overfitting , decreased statistical significance.\n",
    "\n",
    "to deal : correlation matrix, heatmap of correlations, calculate the variance inflation factor, introduce penalizations, or PCA. \n",
    "\n",
    "VIF <= 1 not collinear. 1 to 5 , you can ignore and larger than 5 need to deal with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate, fit, predict\n",
    "lin_mod = LinearRegression()\n",
    "lin_mod.fit(X_train,y_train)\n",
    "y_pred = lin_mod.predict(X_test)\n",
    "\n",
    "# Coefficient estimates\n",
    "print('Coefficients: \\n', lin_mod.coef_)\n",
    "\n",
    "# Mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test,y_pred))\n",
    "\n",
    "# Explained variance score\n",
    "print('R_squared score: %.2f' % r2_score(y_test,y_pred))\n",
    "\n",
    "\n",
    "# Correlation matrix\n",
    "diab_corr = diabetes.corr()\n",
    "\n",
    "# Generate correlation heatmap\n",
    "ax = sns.heatmap(diab_corr, center=0, cmap=sns.diverging_palette(20,220, n=256), square=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()\n",
    "\n",
    "# Print correlations\n",
    "print(diab_corr)\n",
    "\n",
    "\n",
    "# Feature engineering\n",
    "diabetes['s1_s2'] = diabetes['s1'] * diabetes['s2']\n",
    "diabetes = diabetes.drop(['s1','s2'], axis=1)\n",
    "\n",
    "# Print variable names\n",
    "print(diabetes.columns)\n",
    "\n",
    "# Train/test split\n",
    "X2 = diabetes.drop('progression', axis=1)\n",
    "y2 = diabetes['progression']\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.3, random_state=123)\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "lin_mod2 = LinearRegression()\n",
    "lin_mod2.fit(X_train2,y_train2)\n",
    "y_pred2 = lin_mod2.predict(X_test2)\n",
    "\n",
    "# Coefficient estimates\n",
    "print('Coefficients: \\n', lin_mod2.coef_)\n",
    "\n",
    "# Mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test2, y_pred2))\n",
    "\n",
    "# Explained variance score\n",
    "print('R_squared score: %.2f' % r2_score(y_test2, y_pred2))\n",
    "\n",
    "#PCA\n",
    "\n",
    "# Import\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Instantiate\n",
    "pca = PCA()\n",
    "\n",
    "# Fit on train\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Transform train and test\n",
    "X_trainPCA = pca.transform(X_train)\n",
    "X_testPCA = pca.transform(X_test)\n",
    "\n",
    "\n",
    "# Import\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "LinRegr = LinearRegression()\n",
    "LinRegr.fit(X_trainPCA, y_train)\n",
    "predictions = LinRegr.predict(X_testPCA)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', LinRegr.coef_)\n",
    "\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\" % mean_squared_error(y_test, predictions))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(y_test, predictions))\n",
    "\n",
    "\n",
    "# Correlation matrix\n",
    "X_trainPCA = pd.DataFrame(X_trainPCA)\n",
    "diab_corrPCA = X_trainPCA.corr()\n",
    "\n",
    "# Generate correlation heatmap\n",
    "ax = sns.heatmap(diab_corrPCA, center=0, cmap=sns.diverging_palette(20,220, n=256), square=True)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "plt.show()\n",
    "\n",
    "# Print correlations\n",
    "print(diab_corrPCA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "rf_model = RandomForestClassifier(n_estimators=50, random_state=123, oob_score = True)\n",
    "rf_model = rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Random Forest Accuracy: {}\".format(accuracy_score(y_test, rf_pred)))\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, rf_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, rf_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, rf_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, rf_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Instantiate, fit, predict\n",
    "gb_model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.01,random_state=123)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Gradient Boosting Accuracy: {}\".format(accuracy_score(y_test, gb_pred)))\n",
    "print(\"Confusion matrix:\\n {}\".format(confusion_matrix(y_test, gb_pred)))\n",
    "print(\"Precision: {}\".format(precision_score(y_test, gb_pred)))\n",
    "print(\"Recall: {}\".format(recall_score(y_test, gb_pred)))\n",
    "print(\"F1: {}\".format(f1_score(y_test, gb_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
